{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment évoluent les relations entre les personnages de la saga Harry Potter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adrien Vallette et Sophie Vaillant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La saga Harry Potter est composée de 7 ouvrages mettant en scène une multitude de personnages. Au cours de ce projet, nous analyserons les interactions entre ces derniers, principalement en prenant pour référentiel Harry Potter, pour constater leur évolution quantitative et qualitative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Packages et modules que nous utiliserons au cours de ce projet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from pywaffle import Waffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.collocations import *\n",
    "try:\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "import csv\n",
    "import io\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons téléchargé les textes en anglais sur le lien suivant: https://github.com/formcept/whiteboard/tree/master/nbviewer/notebooks/data/harrypotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changement du format des textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tout d'abord, on crée un dictionnaire des différents tomes\n",
    "\n",
    "Books = {'The Philosophers Stone': '1',\n",
    "        'The Chamber of Secrets': '2',\n",
    "        'The Prisoner of Azkaban': '3',\n",
    "        'The Goblet of Fire':'4',\n",
    "        'The Order of the Phoenix': '5',\n",
    "        'The Half Blood Prince': '6',\n",
    "        'The Deathly Hallows': '7'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du CSV (pour Adrien)\n",
    "\n",
    "def creation_csv(nom_livre,numero_livre):\n",
    "    chemin = r\"C:\\Users\\adxva\\OneDrive\\Bureau\\ENSAE 2A - S1\\Harry-Python\\Data\\Books txt\\Book \" + numero_livre + \" - \"  + nom_livre + \".txt\"\n",
    "    sortie = r\"C:\\Users\\adxva\\OneDrive\\Bureau\\ENSAE 2A - S1\\Harry-Python\\Data\\Books CSV\\Book \" + numero_livre + \".csv\"\n",
    "    with io.open(chemin,\"r\",encoding=\"utf-8\") as infile, open(sortie, 'w',encoding = 'utf-8-sig') as outfile:\n",
    "        stripped = (line.strip() for line in infile)\n",
    "        lines = (line.split(\",\") for line in stripped if line)\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fait une boucle exécutant la fonction sur chaque élément du dictionnaire (pour Adrien)\n",
    "\n",
    "Books_csv = {}\n",
    "for title, i in Books.items():\n",
    "    creation_csv(title, i)\n",
    "    Books_csv['book_' + i] = pd.read_csv(r\"C:\\Users\\adxva\\OneDrive\\Bureau\\ENSAE 2A - S1\\Harry-Python\\Data\\Books CSV\\Book \" + i + \".csv\",encoding = 'utf-8-sig', sep='delimiter', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du CSV (pour Sophie)\n",
    "\n",
    "def creation_csv(nom_livre,numero_livre):\n",
    "    chemin = r\"C:\\Users\\Sophie\\Harry-Python\\Data\\Book \" + numero_livre + \" - \" + nom_livre + \".txt\"\n",
    "    sortie = r\"C:\\Users\\Sophie\\Harry-Python\\Data\\book\" + numero_livre + \".csv\"\n",
    "    with io.open(chemin,\"r\",encoding=\"utf-8\") as infile, open(sortie, 'w',encoding = 'utf-8-sig') as outfile:\n",
    "        stripped = (line.strip() for line in infile)\n",
    "        lines = (line.split(\",\") for line in stripped if line)\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fait une boucle exécutant la fonction sur chaque élément du dictionnaire (pour Sophie)\n",
    "\n",
    "Books_csv = {}\n",
    "for title, i in Books.items():\n",
    "    creation_csv(title, i)\n",
    "    Books_csv['book' + i] = pd.read_csv(r\"C:\\Users\\Sophie\\Harry-Python\\Data\\book\" + i + \".csv\" ,encoding = 'utf-8-sig', sep='delimiter', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regardons ce que le CSV donne\n",
    "\n",
    "Books_csv['book4'].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée un tableau avec tous les livres\n",
    "\n",
    "df_books = pd.DataFrame(Books_csv.items(), columns = ['Books', 'Text'])\n",
    "df_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nettoyage des textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite supprimer les lignes en fin de pages qui apparaissent systématiquement, du type \"Page 7 Harry Potter and the Philosophers Stone J.K. Rowling\".\n",
    "Pour cela on crée un pattern.\n",
    "\n",
    "Nous avons pu constater en parcourant les textes qu'ils comportaient des fautes d'orthographe (lorsque l'on comparait les titres de chapitres observés aux vrais titres par exemple), et parfois des O à la place des 0 par exemple, ce qui explique par la suite des patterns parfois alambiqués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée les patterns pour éliminer les lignes de fin de page qui reviennent dans chaque livre\n",
    "\n",
    "pattern = [\"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Philosophers Stone[\\s]?-[\\s]?J.K. Rowling\",\n",
    "           \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Chamber of Secrets[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Prisoner of Azkaban[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Goblet of Fire[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[l0-9]?[lOU0-9]?[lOU0-9]?[lOU0-9][\\s]?Harry Potter and the Order of the Phoenix[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Half Blood Prince[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Deathly Hallows[\\s]?-[\\s]?J.K. Rowling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée une fonction permettant de supprimer ces lignes lorsque le pattern est rencontré au cours du texte\n",
    "\n",
    "def clean_page(text, pattern):\n",
    "    for ele in pattern:\n",
    "        liste_regex = re.findall(ele, text, re.IGNORECASE)\n",
    "        for expression in liste_regex:\n",
    "            text = text.replace(expression, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée le même dictionnaire en fusionnant chaque texte du CSV pour pouvoir appliquer la fonction\n",
    "\n",
    "all_text = Books_csv.copy()\n",
    "for i in Books_csv:\n",
    "    all_text[i] = ' '.join([text for text in Books_csv[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On applique la fonction au dictionnaire des livres afin de supprimer les patterns que l'on rencontre\n",
    "\n",
    "for i in all_text:\n",
    "    all_text[i] = clean_page(all_text[i], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise une tokenisation sur le texte pour enlever les mots qui ne nous intéressent pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "\n",
    "all_text_clean = all_text.copy()\n",
    "\n",
    "for i in all_text:\n",
    "    \n",
    "    text_list = nltk.tokenize.word_tokenize(all_text[i])\n",
    "    text_list = [x.lower() for x in text_list]\n",
    "    text_clean = [w.lower() for w in text_list if w not in stopwords and w.isalpha()]\n",
    "    text_clean = ' '.join(text.lower() for text in text_clean)\n",
    "    all_text_clean[i] = text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On regarde ce que ça donne sur un bout de texte\n",
    "\n",
    "all_text_clean['book5'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient ainsi les données \"all_text_clean\" qui sont les textes nettoyés et regroupés en un dictionnaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistiques descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut regarder les mots qui reviennent le plus dans toute la saga: on réalise des wordclouds et des graphiques pour chaque tome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création des wordclouds\n",
    "\n",
    "for i in all_text_clean:\n",
    "    wordcloud = WordCloud(width=800, height=500,\n",
    "                      random_state=21, max_font_size=110).generate(all_text_clean[i])\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.title(i)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée une fonction qui détermine les mots qui reviennent le plus dans un texte ainsi que leur nombre d'apparition\n",
    "\n",
    "def most_common_word(text_clean):\n",
    "    text_clean_list = text_clean.split()\n",
    "    text_counts = Counter(text_clean_list)\n",
    "    text_common_words = [word[0] for word in text_counts.most_common(25)]\n",
    "    text_common_counts = [word[1] for word in text_counts.most_common(25)]\n",
    "\n",
    "    return text_common_words, text_common_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un dictionnaire avec les mots les plus utilisés par tome\n",
    "\n",
    "hp_most_common = {}\n",
    "for i in all_text_clean:\n",
    "    hp_most_common[i] = most_common_word(all_text_clean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On réalise les graphiques des mots les plus communs\n",
    "\n",
    "for i in hp_most_common:\n",
    "        plt.style.use('dark_background')\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        words = hp_most_common[i][0]\n",
    "        count = hp_most_common[i][1]\n",
    "        sns.barplot(x = words, y = count)\n",
    "        plt.title('Most Common Words used by J.K. Rowling in ' +i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant travailler plus spécifiquement sur les personnages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_clean_list = all_text_clean.copy()\n",
    "for i in all_text_clean:\n",
    "    all_text_clean_list[i] = all_text_clean[i].split()\n",
    "\n",
    "\n",
    "harry = ['harry', 'harry potter']\n",
    "ron = ['ron', 'ron weasley']\n",
    "hermione = ['hermione granger','hermione']\n",
    "ginny = ['ginny weasley', 'ginny']\n",
    "dumbledore = ['albus dumbledore', 'dumbledore']\n",
    "sirius = ['sirius black', 'sirius']\n",
    "snape = ['severus snape', 'snape']\n",
    "luna = ['luna lovegood', 'luna']\n",
    "hagrid = ['rubeus hagrid', 'hagrid']\n",
    "dobby = ['dobby']\n",
    "voldemort = ['tom riddle', 'voldemort', 'you-know-who', 'know-who', 'lord']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
