{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import time\n",
    "# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "from pywaffle import Waffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.collocations import *\n",
    "try:\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "#stopwords\n",
    "import csv\n",
    "import io\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour Sophie\n",
    "def creation_csv(nom_livre,numero_livre):\n",
    "    chemin = r\"C:\\Users\\Sophie\\Harry-Python\\Data\\Book \" + numero_livre + \" - \" + nom_livre + \".txt\"\n",
    "    sortie = r\"C:\\Users\\Sophie\\Harry-Python\\Data\\book\" + numero_livre + \".csv\"\n",
    "    with io.open(chemin,\"r\",encoding=\"utf-8\") as infile, open(sortie, 'w',encoding = 'utf-8-sig') as outfile:\n",
    "        stripped = (line.strip() for line in infile)\n",
    "        lines = (line.split(\",\") for line in stripped if line)\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On crée un dictionnaire\n",
    "Books = {'The Philosophers Stone': '1',\n",
    "        'The Chamber of Secrets': '2',\n",
    "        'The Prisoner of Azkaban': '3',\n",
    "        'The Goblet of Fire':'4',\n",
    "        'The Order of the Phoenix': '5',\n",
    "        'The Half Blood Prince': '6',\n",
    "        'The Deathly Hallows': '7'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sophie\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Pour Sophie\n",
    "#On fait une boucle exécutant la fonction sur chaque élément du dictionnaire\n",
    "Books_csv = {}\n",
    "for title, i in Books.items():\n",
    "    creation_csv(title, i)\n",
    "    Books_csv['book' + i] = pd.read_csv(r\"C:\\Users\\Sophie\\Harry-Python\\Data\\book\" + i + \".csv\" ,encoding = 'utf-8-sig', sep='delimiter', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Books</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book1</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book2</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book3</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book4</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book5</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>book6</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>book7</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Books                                               Text\n",
       "0  book1                                                ...\n",
       "1  book2                                                ...\n",
       "2  book3                                                ...\n",
       "3  book4                                                ...\n",
       "4  book5                                                ...\n",
       "5  book6                                                ...\n",
       "6  book7                                                ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Créer un tableau avec tous les livres\n",
    "df_books = pd.DataFrame(Books_csv.items(), columns = ['Books', 'Text'])\n",
    "df_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On veut éliminer le numéro de page hyper relou\n",
    "import re\n",
    "pattern = [\"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Philosophers Stone[\\s]?-[\\s]?J.K. Rowling\",\n",
    "           \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Chamber of Secrets[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Prisoner of Azkaban[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Goblet of Fire[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[l0-9]?[lOU0-9]?[lOU0-9]?[lOU0-9][\\s]?Harry Potter and the Order of the Phoenix[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Half Blood Prince[\\s]?-[\\s]?J.K. Rowling\",\n",
    "          \"Page[\\s]?\\|[\\s]?[0-9]?[0-9]?[0-9]?[0-9][\\s]?Harry Potter and the Deathly Hallows[\\s]?-[\\s]?J.K. Rowling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_page(text, pattern):\n",
    "    for ele in pattern:\n",
    "        liste_regex = re.findall(ele, text, re.IGNORECASE)\n",
    "        for expression in liste_regex:\n",
    "            text = text.replace(expression, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création du même dictionnaire mais avec texte fusionné pour chaque tome\n",
    "all_text = Books_csv.copy()\n",
    "for i in Books_csv:\n",
    "    all_text[i] = ' '.join([text for text in Books_csv[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Application de la fonction au dictionnaire des livres\n",
    "for i in all_text:\n",
    "    all_text[i] = clean_page(all_text[i], pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install jyquickhelper\n",
    "#from jyquickhelper import add_notebook_menu\n",
    "#add_notebook_menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Sophie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "#print(tweet_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sophie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sophie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
